<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Thinking-Lab-XMU</title>
    <link>http://localhost:1313/</link>
      <atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <description>Thinking-Lab-XMU</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>zh-Hans</language><lastBuildDate>Mon, 24 Oct 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>http://localhost:1313/media/icon_hu_27f5d0c9bdc39eff.png</url>
      <title>Thinking-Lab-XMU</title>
      <link>http://localhost:1313/</link>
    </image>
    
    <item>
      <title>课题组1篇论文被 ICCV2025 录用</title>
      <link>http://localhost:1313/post/3/</link>
      <pubDate>Thu, 26 Jun 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/3/</guid>
      <description>&lt;p&gt;  ICCV会议（International Conference on Computer Vision，计算机视觉国际大会），是是计算机视觉领域的全球顶级学术会议之一，在CCF学术推荐列表中认定为A类会议。ICCV2025 将于2025年10月19日至23日在夏威夷檀香山举行。&lt;/p&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;论文标题：Supervised Exploratory Learning for Long-Tailed Visual Recognition&lt;/li&gt;
&lt;li&gt;录用类型：ICCV2025 主会长文&lt;/li&gt;
&lt;li&gt;论文作者：Zhongquan Jian+, Yanhao Chen+, Yancheng Wang, Junfeng Yao*, Meihong Wang*, Qingqiang Wu*&lt;/li&gt;
&lt;li&gt;完成单位：厦门大学
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /post/3/1_hu_31754d331677aeac.webp 400w,
               /post/3/1_hu_d648bdad1c761587.webp 760w,
               /post/3/1_hu_7ec92eb94ad11baa.webp 1200w&#34;
               src=&#34;http://localhost:1313/post/3/1_hu_31754d331677aeac.webp&#34;
               width=&#34;760&#34;
               height=&#34;398&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;论文简介：
长尾数据对深度学习模型提出了重大挑战，这些模型往往倾向于优先保证头部类别的分类准确性，而严重忽略尾部类别。现有技术（如类别重平衡、逻辑调整和数据增强）主要致力于扩大尾部类别的决策区域或实现清晰的决策边界，却忽视了决策区域的鲁棒性。本文提出了一种简单而有效的监督式探索学习框架（SEL），从空间探索的角度同时实现上述目标。SEL采用自适应最优觅食算法（OFA）生成多样化的探索样本，并结合类别偏置补偿（CbC）实现类别分布平衡，以及适应度加权采样（FwS）进行空间探索。理论分析和实验结果均表明，SEL能够提升类别平衡性、锐化决策边界并强化决策区域。SEL是一种即插即用的训练框架，可无缝集成到模型训练或分类器调整阶段，具有高度适应性和兼容性，能够与现有方法结合并进一步提升性能。在多个长尾基准数据集上的大量实验证明了SEL的优越性。&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Supervised Exploratory Learning for Long-Tailed Visual Recognition</title>
      <link>http://localhost:1313/publication/2/</link>
      <pubDate>Thu, 29 May 2025 08:15:18 +0000</pubDate>
      <guid>http://localhost:1313/publication/2/</guid>
      <description></description>
    </item>
    
    <item>
      <title>DTCRS: Dynamic Tree Construction for Recursive Summarization</title>
      <link>http://localhost:1313/publication/1/</link>
      <pubDate>Thu, 29 May 2025 08:15:12 +0000</pubDate>
      <guid>http://localhost:1313/publication/1/</guid>
      <description></description>
    </item>
    
    <item>
      <title>课题组1篇论文被 Neural Networks 录用</title>
      <link>http://localhost:1313/post/2/</link>
      <pubDate>Fri, 02 May 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/2/</guid>
      <description>&lt;p&gt;  Neural Networks是国际著名的学术期刊，专注于神经网络及深度学习领域的前沿研究，在CCF学术推荐列表中认定为B类期刊。&lt;/p&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;论文标题：Towards Better Text Image Machine Translation with Multimodal Codebook and Multi-stage Training&lt;/li&gt;
&lt;li&gt;录用类型：Neural Networks&lt;/li&gt;
&lt;li&gt;论文作者：Zhibin Lan, Jiawei Yu, Shiyu Liu, Junfeng Yao, Degen Huang, Jinsong Su*&lt;/li&gt;
&lt;li&gt;完成单位：厦门大学，大连理工大学
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /post/2/1_hu_5ac5228311c62ff4.webp 400w,
               /post/2/1_hu_c28cfd52aec1c8cf.webp 760w,
               /post/2/1_hu_49eea571f0317f07.webp 1200w&#34;
               src=&#34;http://localhost:1313/post/2/1_hu_5ac5228311c62ff4.webp&#34;
               width=&#34;760&#34;
               height=&#34;576&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;论文简介：
当前图文翻译任务在跨模态理解与语言生成领域受到广泛关注，但现有方法仍面临两大核心挑战：一方面，主流方法普遍采用OCR识别与文本翻译串联的级联结构，导致OCR误识别会严重影响最终翻译结果；另一方面，缺乏大规模、高质量的公开图文翻译数据集也限制了模型能力的进一步提升。为此，本研究人工标注并公开了首个中英图文翻译数据集OCRMT30K，并借助自动翻译工具将其扩展到中德语言对上，为该领域提供了宝贵的训练资源与评测基准。此外，本研究提出了一种基于多模态码本的图文翻译模型，通过引入图像编码器、文本编码器、文本解码器以及可桥接图文语义的多模态代码本，实现了跨模态语义的高效对齐与增强的翻译性能。同时，论文设计了一套多阶段训练框架，充分利用不同类型的数据资源，逐步优化各个模块：先基于双语文本进行文本模块的预训练，接着引入基于码元的掩码翻译任务进一步训练多模态码本与文本编码器和解码器模块，再借助图文对齐与对抗训练方法在OCR数据集上优化图像编码器模块与多模态码本，最后使用图文翻译数据集对整个模型进行微调。实验结果表明，该模型在中英与中德图文翻译任务中均显著优于现有方法，验证了其跨模态建模与阶段训练策略的有效性。&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>课题组1篇论文被 IJCAI2025 录用</title>
      <link>http://localhost:1313/post/1/</link>
      <pubDate>Thu, 01 May 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/1/</guid>
      <description>&lt;p&gt;  IJCAI会议（International Joint Conference on Artificial Intelligence，国际人工智能联合会议）是人工智能领域最具历史和学术声望的国际会议之一，在CCF学术推荐列表中认定为A类会议。IJCAI 2025将于2025年8月在加拿大蒙特利尔举办。&lt;/p&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;论文标题：Boosting Visual Knowledge-Intensive Training for LVLMs through Causality-driven Visual Object Completion&lt;/li&gt;
&lt;li&gt;录用类型：IJCAI2025&lt;/li&gt;
&lt;li&gt;论文作者：Qingguo Hu+, Ante Wang+, Jia Song, Delai Qiu, Qingsong Liu, Jinsong Su*&lt;/li&gt;
&lt;li&gt;完成单位：厦门大学，云知声
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /post/1/1_hu_488e26ebb80fa4e2.webp 400w,
               /post/1/1_hu_5ea39c84ad1ed0ac.webp 760w,
               /post/1/1_hu_6765f995d73d88f9.webp 1200w&#34;
               src=&#34;http://localhost:1313/post/1/1_hu_488e26ebb80fa4e2.webp&#34;
               width=&#34;564&#34;
               height=&#34;491&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;论文简介：
本研究提出了一种创新的自我改进框架，用于增强视觉语言模型（LVLMs）的视觉感知与推理能力。该框架基于因果驱动的视觉对象补全任务（CVC），要求以明确的推理链形式，利用图像中的可见上下文信息来推断被遮挡的对象，从而将感知密集的复杂推理能力引入LVLMs。类似于人类通过反复试错来提升解决复杂问题的能力，该框架采用试错学习来强化LVLM对CVC的掌握程度，从而提升其全面的视觉能力。首先，该框架采样LVLM的多个推理路径（试验），然后挑选出对训练有价值的样本，最终将这些自我生成的试验用于LVLM的自我改进。因此，LVLM的视觉能力可以在不依赖人类或更先进 LVLM 的情况下得到全面的自我提升。实验证明，该框架在多个通用测试基准和高难度专项任务上均优于对应的基线模型，尤其在更具挑战性的任务中，如MMVP和Winoground，分别实现了10.0%和8.2%的提升。&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Contact</title>
      <link>http://localhost:1313/contact/</link>
      <pubDate>Mon, 24 Oct 2022 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/contact/</guid>
      <description></description>
    </item>
    
    <item>
      <title>学术团队</title>
      <link>http://localhost:1313/people/</link>
      <pubDate>Mon, 24 Oct 2022 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/people/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Dependency-Based Bracketing Transduction Grammar for Statistical Machine Translation</title>
      <link>http://localhost:1313/project/1/</link>
      <pubDate>Fri, 28 May 2010 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/project/1/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:1313/admin/config.yml</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/admin/config.yml</guid>
      <description></description>
    </item>
    
    <item>
      <title>研途印象</title>
      <link>http://localhost:1313/environment/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/environment/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
